{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a21f3b6",
   "metadata": {},
   "source": [
    "# Logic gates using neural networks via tf.keras\n",
    "https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\n",
    "\n",
    "Here we are using 'tf.keras' as recommended. \n",
    "\n",
    "https://www.pyimagesearch.com/2019/10/21/keras-vs-tf-keras-whats-the-difference-in-tensorflow-2-0/\n",
    "\n",
    "This is essentially, keras being included as a submodule inside tensorflow itself.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0242a15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164ce833",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d3ef9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "inputs = np.array([[0.,0.,1.,1.],[0.,1.,0.,1.]]).T\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b44117",
   "metadata": {},
   "source": [
    "## Expected outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f32d4553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AND function\n",
    "outputAND = np.array([0.,0.,0.,1.])\n",
    "outputAND = np.asarray([outputAND]).T\n",
    "# OR function\n",
    "outputOR = np.array([0.,1.,1.,1.])\n",
    "outputOR = np.asarray([outputOR]).T\n",
    "# NAND function\n",
    "outputNAND = np.array([1.,1.,1.,0.])\n",
    "outputNAND = np.asarray([outputNAND]).T\n",
    "# XOR function\n",
    "outputXOR = np.array([0.,1.,1.,1.])\n",
    "outputXOR = np.asarray([outputXOR]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fff7ae3",
   "metadata": {},
   "source": [
    "## Set initial weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd1f3e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights matrices:  [array([[0.3 , 0.45],\n",
      "       [0.55, 0.5 ],\n",
      "       [0.2 , 0.35]]), array([[0.15, 0.4 , 0.25]])]\n",
      "Biases:  [array([0.6, 0.6, 0.6]), array([0.05])]\n"
     ]
    }
   ],
   "source": [
    "# Initial guesses for weights\n",
    "w1 = 0.30\n",
    "w2 = 0.55\n",
    "w3 = 0.20\n",
    "w4 = 0.45\n",
    "w5 = 0.50\n",
    "w6 = 0.35\n",
    "w7 = 0.15\n",
    "w8 = 0.40\n",
    "w9 = 0.25\n",
    "\n",
    "# Initial guesses for biases\n",
    "b1 = 0.60\n",
    "b2 = 0.05\n",
    "\n",
    "# need to use a list instead of a numpy array, since the \n",
    "#weight matrices at each layer are not of the same dimensions\n",
    "weights = [] \n",
    "# Weights for layer 1 --> 2\n",
    "weights.append(np.array([[w1,w4],[w2, w5], [w3, w6]]))\n",
    "# Weights for layer 2 --> 3\n",
    "weights.append(np.array([[w7, w8, w9]]))\n",
    "# List of biases at each layer\n",
    "biases = []\n",
    "biases.append(np.array([b1,b1,b1]))\n",
    "biases.append(np.array([b2]))\n",
    "\n",
    "weightsOriginal = weights\n",
    "biasesOriginal = biases\n",
    "\n",
    "print('Weights matrices: ',weights)\n",
    "print('Biases: ',biases)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0f0285",
   "metadata": {},
   "source": [
    "## Some more settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "136c8c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nLayers = 2\n",
    "nSamples = 4\n",
    "eeta = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b73df8a",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6dd42b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(3, input_dim=2, activation='sigmoid', use_bias=True))\n",
    "model.add(Dense(1, activation='sigmoid', use_bias=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497834c9",
   "metadata": {},
   "source": [
    "## Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "963de0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 3)                 9         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 4         \n",
      "=================================================================\n",
      "Total params: 13\n",
      "Trainable params: 13\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df4a6ae",
   "metadata": {},
   "source": [
    "## Check the initial weights and biases for each layer\n",
    "\n",
    "Note how the weights matrix is not 3x2 but rather 2x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f5fafa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Weights for layer  1\n",
      "[[-0.45773363 -0.84572554  0.38600314]\n",
      " [-0.53165615 -0.2992524   0.24187028]]\n",
      "\n",
      " Biases for layer  1\n",
      "[0. 0. 0.]\n",
      "\n",
      " Weights for layer  2\n",
      "[[ 0.65574324]\n",
      " [ 0.8888172 ]\n",
      " [-1.047615  ]]\n",
      "\n",
      " Biases for layer  2\n",
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "for i in range(nLayers):\n",
    "    print('\\n Weights for layer ',i+1)\n",
    "    print(model.layers[i].get_weights()[0])\n",
    "    print('\\n Biases for layer ',i+1)\n",
    "    print(model.layers[i].get_weights()[1])\n",
    "# model.layers[0].get_biases()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5f3fd1",
   "metadata": {},
   "source": [
    "## Change initial weights and biases for each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4577d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 1\n",
    "model.layers[0].set_weights([weightsOriginal[0].T, biasesOriginal[0]])\n",
    "\n",
    "# Layer 2\n",
    "model.layers[1].set_weights([weightsOriginal[1].T, biasesOriginal[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90554d39",
   "metadata": {},
   "source": [
    "## Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4404854e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the keras model\n",
    "\n",
    "# In the following manner we can't set the learning rate of the optimizer\n",
    "# model.compile(loss='mse', optimizer='sgd', metrics=['mse'])\n",
    "\n",
    "# So use the following instead\n",
    "model.compile(loss='mse', optimizer=optimizers.SGD(learning_rate=0.5), metrics=['mse'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4463fc12",
   "metadata": {},
   "source": [
    "## Forward feed\n",
    "\n",
    "Note: Batch size=4 indicates that we are using all the training data(examples) and therefore this is the equivalent of my own implementation as I don't have stochastic/minibatch gradient descent implementation yet.\n",
    "\n",
    "References: https://machinelearningmastery.com/how-to-control-the-speed-and-stability-of-training-neural-networks-with-gradient-descent-batch-size/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ba4b57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "4/1 [========================================================================================================================] - 0s 30ms/sample - loss: 0.3434 - mse: 0.3434\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3434138894081116, 0.3434139]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(inputs, outputAND, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6621335",
   "metadata": {},
   "source": [
    "## Does tf.model.evaluate change the weights and biases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0640bf71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Weights for layer  1\n",
      "[[0.3  0.55 0.2 ]\n",
      " [0.45 0.5  0.35]]\n",
      "\n",
      " Biases for layer  1\n",
      "[0.6 0.6 0.6]\n",
      "\n",
      " Weights for layer  2\n",
      "[[0.15]\n",
      " [0.4 ]\n",
      " [0.25]]\n",
      "\n",
      " Biases for layer  2\n",
      "[0.05]\n"
     ]
    }
   ],
   "source": [
    "for i in range(nLayers):\n",
    "    print('\\n Weights for layer ',i+1)\n",
    "    print(model.layers[i].get_weights()[0])\n",
    "    print('\\n Biases for layer ',i+1)\n",
    "    print(model.layers[i].get_weights()[1])\n",
    "# model.layers[0].get_biases()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0b0b52",
   "metadata": {},
   "source": [
    "From the above, we can be sure that it does not change the weights and biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03a0da1",
   "metadata": {},
   "source": [
    "## Fit 1 epoch  (forward feed, backpropagation, updating the weights, biases)\n",
    "\n",
    "Let us just try to see and compare the error after just 1 epoch. \n",
    "\n",
    "We should expect the model to perform forward feed, calculate loss/error,\n",
    "perform backpropagation,\n",
    "and adjust the weights and biases based on the learning rate.\n",
    "\n",
    "Note: Batch size=4 indicates that we are using all the training data(examples) and therefore this is the equivalent of my own implementation as I don't have stochastic/minibatch gradient descent implementation yet.\n",
    "\n",
    "References: https://machinelearningmastery.com/how-to-control-the-speed-and-stability-of-training-neural-networks-with-gradient-descent-batch-size/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "99b06019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4 samples\n",
      "4/4 [==============================] - 0s 118ms/sample - loss: 0.3434 - mse: 0.3434\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x141d234d0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the keras model on the dataset\n",
    "model.fit(inputs, outputAND, epochs=1, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea5b430",
   "metadata": {},
   "source": [
    "## Now the weights and biases must have been updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8b43c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Weights for layer  1\n",
      "[[0.29931322 0.54829705 0.1988662 ]\n",
      " [0.4493876  0.49822634 0.34898397]]\n",
      "\n",
      " Biases for layer  1\n",
      "[0.5969832 0.5921526 0.5948988]\n",
      "\n",
      " Weights for layer  2\n",
      "[[0.08719039]\n",
      " [0.33587077]\n",
      " [0.1880536 ]]\n",
      "\n",
      " Biases for layer  2\n",
      "[-0.04234041]\n"
     ]
    }
   ],
   "source": [
    "for i in range(nLayers):\n",
    "    print('\\n Weights for layer ',i+1)\n",
    "    print(model.layers[i].get_weights()[0])\n",
    "    print('\\n Biases for layer ',i+1)\n",
    "    print(model.layers[i].get_weights()[1])\n",
    "# model.layers[0].get_biases()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828c93f2",
   "metadata": {},
   "source": [
    "## Now let us do a forward feed again and calculate the loss/error\n",
    "\n",
    "Note: Batch size=4 indicates that we are using all the training data(examples) and therefore this is the equivalent of my own implementation as I don't have stochastic/minibatch gradient descent implementation yet.\n",
    "\n",
    "References: https://machinelearningmastery.com/how-to-control-the-speed-and-stability-of-training-neural-networks-with-gradient-descent-batch-size/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28ad2a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/1 [========================================================================================================================] - 0s 609us/sample - loss: 0.3041 - mse: 0.3041\n",
      "[0.30411168932914734, 0.3041117]\n"
     ]
    }
   ],
   "source": [
    "out = model.evaluate(inputs, outputAND, batch_size=4)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec99b920",
   "metadata": {},
   "source": [
    "### The above result, compares well with the PyTorch result as well as the result from my own implementation (when biases are updated independently)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8326396",
   "metadata": {},
   "source": [
    "## Now let us let the model train for 10^4 epochs\n",
    "\n",
    "Note: Batch size=4 indicates that we are using all the training data(examples) and therefore this is the equivalent of my own implementation as I don't have stochastic/minibatch gradient descent implementation yet.\n",
    "\n",
    "References: https://machinelearningmastery.com/how-to-control-the-speed-and-stability-of-training-neural-networks-with-gradient-descent-batch-size/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ce71100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.6 s, sys: 6.03 s, total: 44.7 s\n",
      "Wall time: 26.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# fit the keras model on the dataset\n",
    "history = model.fit(inputs, outputAND, epochs=10**4, batch_size=4, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0bc271",
   "metadata": {},
   "source": [
    "Turns out that this was quite slow. At first, I thought that the problem was that it was printing at each epoch. So I set the verbose=0 (silent). But still, it was incredibly slow compared to my implementation as well as PyTorch, even though we used own for loop in both the cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8281b61b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.29656221345067024,\n",
       "  0.23853549733757973,\n",
       "  0.2210054136812687,\n",
       "  0.2138444073498249,\n",
       "  0.21007107011973858,\n",
       "  0.20859628729522228,\n",
       "  0.20775542315095663,\n",
       "  0.20478377677500248,\n",
       "  0.20751046016812325,\n",
       "  0.2044722056016326,\n",
       "  0.20712593756616116,\n",
       "  0.20543506834656,\n",
       "  0.20548744313418865,\n",
       "  0.20535280648618937,\n",
       "  0.20622337143868208,\n",
       "  0.20352253038436174,\n",
       "  0.20606463961303234,\n",
       "  0.2045597555115819,\n",
       "  0.20542436838150024,\n",
       "  0.2050496507436037,\n",
       "  0.20461109932512045,\n",
       "  0.2036317829042673,\n",
       "  0.20374681241810322,\n",
       "  0.2023001816123724,\n",
       "  0.2039965558797121,\n",
       "  0.2046814002096653,\n",
       "  0.20429792441427708,\n",
       "  0.20171229168772697,\n",
       "  0.20431876368820667,\n",
       "  0.20386942476034164,\n",
       "  0.2025942075997591,\n",
       "  0.20337327476590872,\n",
       "  0.200950532220304,\n",
       "  0.20339590031653643,\n",
       "  0.20306825265288353,\n",
       "  0.2028009295463562,\n",
       "  0.20043280720710754,\n",
       "  0.20047559216618538,\n",
       "  0.2030652090907097,\n",
       "  0.20181972905993462,\n",
       "  0.2023759577423334,\n",
       "  0.20189476292580366,\n",
       "  0.20090340171009302,\n",
       "  0.2015999285504222,\n",
       "  0.20062631648033857,\n",
       "  0.2013244591653347,\n",
       "  0.20097653567790985,\n",
       "  0.2007332043722272,\n",
       "  0.1983887730166316,\n",
       "  0.1983720799908042,\n",
       "  0.20070951152592897,\n",
       "  0.19815041404217482,\n",
       "  0.20044981129467487,\n",
       "  0.200047817081213,\n",
       "  0.19758864119648933,\n",
       "  0.19994681794196367,\n",
       "  0.19854384660720825,\n",
       "  0.1992627689614892,\n",
       "  0.19873031601309776,\n",
       "  0.19845228735357523,\n",
       "  0.19821655191481113,\n",
       "  0.19792911689728498,\n",
       "  0.19566207379102707,\n",
       "  0.19757706858217716,\n",
       "  0.19513312447816133,\n",
       "  0.19708573259413242,\n",
       "  0.19675496965646744,\n",
       "  0.19573086127638817,\n",
       "  0.19556902069598436,\n",
       "  0.1954083452001214,\n",
       "  0.19560765847563744,\n",
       "  0.19526074547320604,\n",
       "  0.19278946612030268,\n",
       "  0.1926342323422432,\n",
       "  0.1947049144655466,\n",
       "  0.19403351470828056,\n",
       "  0.19331446662545204,\n",
       "  0.19230608269572258,\n",
       "  0.1925191842019558,\n",
       "  0.18998843058943748,\n",
       "  0.1915138941258192,\n",
       "  0.19048610981553793,\n",
       "  0.19059787318110466,\n",
       "  0.18993033841252327,\n",
       "  0.18937278725206852,\n",
       "  0.18689875770360231,\n",
       "  0.1880472358316183,\n",
       "  0.1878295075148344,\n",
       "  0.18704816233366728,\n",
       "  0.18545212596654892,\n",
       "  0.18528616894036531,\n",
       "  0.18316092900931835,\n",
       "  0.18412286415696144,\n",
       "  0.18351506907492876,\n",
       "  0.18251037131994963,\n",
       "  0.17980866599828005,\n",
       "  0.18068952485919,\n",
       "  0.17975110094994307,\n",
       "  0.17860743310302496,\n",
       "  0.17748876102268696,\n",
       "  0.17698716465383768,\n",
       "  0.17536794301122427,\n",
       "  0.1727233133278787,\n",
       "  0.17348490562289953,\n",
       "  0.17237550672143698,\n",
       "  0.17035721708089113,\n",
       "  0.169572239741683,\n",
       "  0.1676649246364832,\n",
       "  0.16646139370277524,\n",
       "  0.16503335535526276,\n",
       "  0.16401786310598254,\n",
       "  0.16062213433906436,\n",
       "  0.1599441710859537,\n",
       "  0.15921351965516806,\n",
       "  0.15573021536692977,\n",
       "  0.1558540239930153,\n",
       "  0.1530864266678691,\n",
       "  0.15234737331047654,\n",
       "  0.1489660469815135,\n",
       "  0.1491678818129003,\n",
       "  0.14718300802633166,\n",
       "  0.1433447482995689,\n",
       "  0.1419757679104805,\n",
       "  0.14079668326303363,\n",
       "  0.13847300736233592,\n",
       "  0.13742164382711053,\n",
       "  0.1351717603392899,\n",
       "  0.13272370724007487,\n",
       "  0.12959100655280054,\n",
       "  0.1283376240171492,\n",
       "  0.12548507121391594,\n",
       "  0.12329371832311153,\n",
       "  0.1218728618696332,\n",
       "  0.12047048821114004,\n",
       "  0.11892373533919454,\n",
       "  0.11518584343139082,\n",
       "  0.11310958885587752,\n",
       "  0.1109154901932925,\n",
       "  0.10887998528778553,\n",
       "  0.10837598610669374,\n",
       "  0.10507506807334721,\n",
       "  0.10434003116097301,\n",
       "  0.10201396082993597,\n",
       "  0.09986501710955054,\n",
       "  0.09649448876734823,\n",
       "  0.09498252649791539,\n",
       "  0.09332801960408688,\n",
       "  0.0911151115433313,\n",
       "  0.08966348762623966,\n",
       "  0.08770320855546743,\n",
       "  0.0843993944581598,\n",
       "  0.08318212436279282,\n",
       "  0.08133427542634308,\n",
       "  0.07923996401950717,\n",
       "  0.07765327673405409,\n",
       "  0.07504302158486098,\n",
       "  0.07375865647918545,\n",
       "  0.07290541793918237,\n",
       "  0.06984631676459685,\n",
       "  0.06876726140035316,\n",
       "  0.06657870294293389,\n",
       "  0.06617629452375695,\n",
       "  0.06381283415248618,\n",
       "  0.062372457585297525,\n",
       "  0.06109407191979699,\n",
       "  0.06015088123967871,\n",
       "  0.057854578422848135,\n",
       "  0.05594587264931761,\n",
       "  0.05551653422298841,\n",
       "  0.053618098303559236,\n",
       "  0.05281867567100562,\n",
       "  0.05061665046378039,\n",
       "  0.05030309400171973,\n",
       "  0.04835262769483961,\n",
       "  0.047503166511887684,\n",
       "  0.04585542334825732,\n",
       "  0.04553934982686769,\n",
       "  0.044359781430102885,\n",
       "  0.04325121369038243,\n",
       "  0.042189615312963724,\n",
       "  0.04085834705620073,\n",
       "  0.039585296472068876,\n",
       "  0.038966389212873764,\n",
       "  0.03774765582056716,\n",
       "  0.037161857973842416,\n",
       "  0.03650847180688288,\n",
       "  0.035426429560175166,\n",
       "  0.03437467323237797,\n",
       "  0.03403939888085006,\n",
       "  0.03288360143778846,\n",
       "  0.032370735199947376,\n",
       "  0.0315804087658762,\n",
       "  0.031072994686837774,\n",
       "  0.03035111443750793,\n",
       "  0.029530989602790214,\n",
       "  0.02905071466375375,\n",
       "  0.028091115196730243,\n",
       "  0.027674362932884833,\n",
       "  0.02719418622291414,\n",
       "  0.026499972416786477,\n",
       "  0.02607519502271316,\n",
       "  0.025514951747027226,\n",
       "  0.024881378863938153,\n",
       "  0.024402483682933962,\n",
       "  0.024021219498536084,\n",
       "  0.023421269332175143,\n",
       "  0.023058509192196652,\n",
       "  0.022501224357256433,\n",
       "  0.022153077559778467,\n",
       "  0.021633047123032156,\n",
       "  0.021241298727545654,\n",
       "  0.020736093267260003,\n",
       "  0.02042123363025894,\n",
       "  0.020054420185260824,\n",
       "  0.019754380653466797,\n",
       "  0.01937476466264343,\n",
       "  0.019017967195395613,\n",
       "  0.018622531673827325,\n",
       "  0.018349612793826964,\n",
       "  0.01802047178898647,\n",
       "  0.017704532592688338,\n",
       "  0.017397483832610305,\n",
       "  0.01709945351285569,\n",
       "  0.016767863889981527,\n",
       "  0.01643083784074406,\n",
       "  0.016170024135135463,\n",
       "  0.015924008745059837,\n",
       "  0.01575431501987623,\n",
       "  0.015447482684976421,\n",
       "  0.015187712291663047,\n",
       "  0.014979391144152032,\n",
       "  0.014712192029037396,\n",
       "  0.014488985068055626,\n",
       "  0.014271862632085686,\n",
       "  0.014058551776543027,\n",
       "  0.013849847527126258,\n",
       "  0.01366664722445421,\n",
       "  0.013426410840111203,\n",
       "  0.013254490771032579,\n",
       "  0.01303109759101062,\n",
       "  0.012810146297852043,\n",
       "  0.012623031433577125,\n",
       "  0.012450219597667456,\n",
       "  0.01230527836469264,\n",
       "  0.012126144341891631,\n",
       "  0.011930765261240595,\n",
       "  0.011812927141363616,\n",
       "  0.011651002157123003,\n",
       "  0.011493123787658988,\n",
       "  0.011300770265734172,\n",
       "  0.011192353884325712,\n",
       "  0.01104268482686166,\n",
       "  0.010863392280043627,\n",
       "  0.010728932178608375,\n",
       "  0.010611381108446949,\n",
       "  0.010467589198469796,\n",
       "  0.010333717808862275,\n",
       "  0.010217182181804674,\n",
       "  0.010089305467772647,\n",
       "  0.009952839357083576,\n",
       "  0.009814783109959535,\n",
       "  0.009712349748497218,\n",
       "  0.009579546903296432,\n",
       "  0.009481223842158215,\n",
       "  0.009378704554819706,\n",
       "  0.009244299372767273,\n",
       "  0.009139083430454775,\n",
       "  0.009036833483150986,\n",
       "  0.00894694446833455,\n",
       "  0.008849472105339373,\n",
       "  0.008740212644170242,\n",
       "  0.008649590093227744,\n",
       "  0.008533718435046467,\n",
       "  0.008459941560886364,\n",
       "  0.008366730234683928,\n",
       "  0.008257772020897391,\n",
       "  0.008187619979253213,\n",
       "  0.008099025023057038,\n",
       "  0.007996172715593275,\n",
       "  0.007922228563529643,\n",
       "  0.007827940249171661,\n",
       "  0.007748438583803363,\n",
       "  0.007670493355362851,\n",
       "  0.007600689710216102,\n",
       "  0.007519665614836413,\n",
       "  0.007434421806465252,\n",
       "  0.007374259340394929,\n",
       "  0.007294772874047339,\n",
       "  0.007227003961816081,\n",
       "  0.007143405156512017,\n",
       "  0.007081608272983431,\n",
       "  0.0070170419144233165,\n",
       "  0.0069492930078922655,\n",
       "  0.0068713840109921875,\n",
       "  0.00681326126868953,\n",
       "  0.006741933019839053,\n",
       "  0.006690017874461773,\n",
       "  0.00662745468639514,\n",
       "  0.006556036355732431,\n",
       "  0.006506347504227961,\n",
       "  0.006442491213647372,\n",
       "  0.006387017386941807,\n",
       "  0.006329329641175718,\n",
       "  0.006263694773451789,\n",
       "  0.006208842914702473,\n",
       "  0.006159425623536663,\n",
       "  0.006104237848603589,\n",
       "  0.006053938169543471,\n",
       "  0.0059981576916925405,\n",
       "  0.005941565728562637,\n",
       "  0.0058986216652101575,\n",
       "  0.005848349082498316,\n",
       "  0.005798753683393443,\n",
       "  0.005747039186189795,\n",
       "  0.00569887668393676,\n",
       "  0.005647277372645476,\n",
       "  0.005604822551276811,\n",
       "  0.005554759490905781,\n",
       "  0.0055161489772217465,\n",
       "  0.005471350504421935,\n",
       "  0.0054271779265491205,\n",
       "  0.005377572882025561,\n",
       "  0.005338617258303202,\n",
       "  0.005298315185427782,\n",
       "  0.005250877361959283,\n",
       "  0.0052157042127873865,\n",
       "  0.005169605144374145,\n",
       "  0.005130107839477205,\n",
       "  0.005091162882536082,\n",
       "  0.005057629652128526,\n",
       "  0.005013894960029575,\n",
       "  0.004979061981657651,\n",
       "  0.004940707709693015,\n",
       "  0.0049032663189336745,\n",
       "  0.004866621659857628,\n",
       "  0.004830609407690645,\n",
       "  0.004792333699924711,\n",
       "  0.004761755307754356,\n",
       "  0.004722701648233851,\n",
       "  0.004692969255074786,\n",
       "  0.004655006289112862,\n",
       "  0.004626027119002174,\n",
       "  0.004589038806443568,\n",
       "  0.004560698044315359,\n",
       "  0.0045246532422424934,\n",
       "  0.004493288411822505,\n",
       "  0.004464321359250789,\n",
       "  0.0044307170746833435,\n",
       "  0.004403966396125725,\n",
       "  0.004370136846432615,\n",
       "  0.004342491372995028,\n",
       "  0.004310598109782404,\n",
       "  0.004285023971533519,\n",
       "  0.004256079304695959,\n",
       "  0.004227489655249883,\n",
       "  0.004197973738087057,\n",
       "  0.004168107634541229,\n",
       "  0.004140866965485657,\n",
       "  0.004116913036796177,\n",
       "  0.004087047697112212,\n",
       "  0.0040623998888804635,\n",
       "  0.004037007276451732,\n",
       "  0.0040110005257929515,\n",
       "  0.003984173098274368,\n",
       "  0.003958668560471779,\n",
       "  0.003931908359618319,\n",
       "  0.0039088217229164,\n",
       "  0.0038842410103825387,\n",
       "  0.0038584234058589573,\n",
       "  0.0038360152477707743,\n",
       "  0.0038123027603660375,\n",
       "  0.003788856354276504,\n",
       "  0.0037666373183355972,\n",
       "  0.003742704040519129,\n",
       "  0.0037200697036041674,\n",
       "  0.0036985460621963284,\n",
       "  0.003676232003044788,\n",
       "  0.0036519013212910068,\n",
       "  0.003632335115639762,\n",
       "  0.0036107857706610957,\n",
       "  0.003587323785723129,\n",
       "  0.0035663288265368465,\n",
       "  0.003546791166627372,\n",
       "  0.0035260561152199443,\n",
       "  0.0035055993449759626,\n",
       "  0.003486155291398063,\n",
       "  0.003466109066835088,\n",
       "  0.003446275299666013,\n",
       "  0.0034247508464204657,\n",
       "  0.003407232747690614,\n",
       "  0.003387993831552194,\n",
       "  0.0033689492206576688,\n",
       "  0.0033500995914437226,\n",
       "  0.0033307359450418517,\n",
       "  0.0033129135540548305,\n",
       "  0.0032929122785390064,\n",
       "  0.0032748810258453886,\n",
       "  0.003257035935462227,\n",
       "  0.0032409692145165536,\n",
       "  0.0032233424271908007,\n",
       "  0.003205259072615263,\n",
       "  0.003188535709568896,\n",
       "  0.003170828554516447,\n",
       "  0.0031538585016051,\n",
       "  0.003137093962095605,\n",
       "  0.003120499223769002,\n",
       "  0.0031046237850205216,\n",
       "  0.003087761734832384,\n",
       "  0.0030707567132139957,\n",
       "  0.003055605016129448,\n",
       "  0.0030397855153978526,\n",
       "  0.0030241146580465283,\n",
       "  0.003007739582130853,\n",
       "  0.0029931141524457416,\n",
       "  0.002977050504341605,\n",
       "  0.002962699131330737,\n",
       "  0.002946937877823075,\n",
       "  0.0029328464986519975,\n",
       "  0.0029181256593346916,\n",
       "  0.0029027933933321037,\n",
       "  0.0028895234264609826,\n",
       "  0.002874716403084676,\n",
       "  0.0028609682530031932,\n",
       "  0.002846869268523733,\n",
       "  0.002831762616551714,\n",
       "  0.002818629248679372,\n",
       "  0.002805335735274639,\n",
       "  0.0027917247952018442,\n",
       "  0.0027782422086488623,\n",
       "  0.002764450571305588,\n",
       "  0.002751609503548025,\n",
       "  0.002737430015542941,\n",
       "  0.0027254503905282945,\n",
       "  0.002712528026165728,\n",
       "  0.002699721983674408,\n",
       "  0.0026870167881156704,\n",
       "  0.0026734602364513194,\n",
       "  0.0026615854553142526,\n",
       "  0.0026495528557006764,\n",
       "  0.002637273604989332,\n",
       "  0.002624726536794242,\n",
       "  0.002613006787839822,\n",
       "  0.002600123104684826,\n",
       "  0.002588813428303638,\n",
       "  0.002577369304333388,\n",
       "  0.0025656801824993636,\n",
       "  0.002553231342062645,\n",
       "  0.0025417701389756076,\n",
       "  0.0025309023478712334,\n",
       "  0.002519891500412541,\n",
       "  0.0025086647939360773,\n",
       "  0.0024972150853841413,\n",
       "  0.002485676899311784,\n",
       "  0.0024747502607169736,\n",
       "  0.0024639187576553923,\n",
       "  0.002453933977278666,\n",
       "  0.0024424764145578592,\n",
       "  0.0024319066540670065,\n",
       "  0.002421419190341112,\n",
       "  0.0024114324432389367,\n",
       "  0.002400581868471363,\n",
       "  0.0023907445411168737,\n",
       "  0.002380490832763371,\n",
       "  0.002370320779618851,\n",
       "  0.002360246979264957,\n",
       "  0.0023502637438923557,\n",
       "  0.002340363361213349,\n",
       "  0.0023305414975425265,\n",
       "  0.0023207970332919103,\n",
       "  0.002311128765484227,\n",
       "  0.002301785145618851,\n",
       "  0.0022919988969078986,\n",
       "  0.002282547122035794,\n",
       "  0.00227317223988166,\n",
       "  0.002263864914880287,\n",
       "  0.0022542297338077333,\n",
       "  0.0022454033931467166,\n",
       "  0.0022359156413358505,\n",
       "  0.002226846433757146,\n",
       "  0.0022184473215247635,\n",
       "  0.00220893967258462,\n",
       "  0.002200672247397506,\n",
       "  0.002191307021519151,\n",
       "  0.0021825980083463037,\n",
       "  0.002174504761114804,\n",
       "  0.0021656838322314798,\n",
       "  0.0021571231757206988,\n",
       "  0.0021486389263714045,\n",
       "  0.0021404266053650645,\n",
       "  0.0021320616914408674,\n",
       "  0.0021237567310095073,\n",
       "  0.0021155117054831862,\n",
       "  0.002106812525994428,\n",
       "  0.0020991994589962815,\n",
       "  0.002091125012327666,\n",
       "  0.0020831074090210677,\n",
       "  0.0020751424809759556,\n",
       "  0.0020667519507924226,\n",
       "  0.0020593983069261412,\n",
       "  0.0020514083625471358,\n",
       "  0.0020438357430521137,\n",
       "  0.002035963135199381,\n",
       "  0.0020285018654817577,\n",
       "  0.0020207285249966844,\n",
       "  0.002013195317658756,\n",
       "  0.0020058840236174547,\n",
       "  0.001998450202790991,\n",
       "  0.0019908954002403334,\n",
       "  0.0019835583910321475,\n",
       "  0.001976443131582073,\n",
       "  0.0019687752358095167,\n",
       "  0.001961847015707008,\n",
       "  0.0019548641265600963,\n",
       "  0.0019476056039025025,\n",
       "  0.0019405638847160844,\n",
       "  0.0019335634797812418,\n",
       "  0.00192636266763202,\n",
       "  0.001919848321392692,\n",
       "  0.0019128257850979935,\n",
       "  0.001905765647336466,\n",
       "  0.0018989847933426063,\n",
       "  0.0018922544913948514,\n",
       "  0.0018858035165294496,\n",
       "  0.0018789257333651221,\n",
       "  0.0018723271853104961,\n",
       "  0.001865771649022463,\n",
       "  0.0018594807825706994,\n",
       "  0.0018531331102735749,\n",
       "  0.0018466890187482932,\n",
       "  0.001839942017852536,\n",
       "  0.0018335918206844326,\n",
       "  0.0018272871169529026,\n",
       "  0.0018213606086874279,\n",
       "  0.0018149776007589935,\n",
       "  0.001808897756575334,\n",
       "  0.001802599486637746,\n",
       "  0.0017965963064341395,\n",
       "  0.0017903778929166947,\n",
       "  0.0017843217011375145,\n",
       "  0.0017781129848799537,\n",
       "  0.0017721442677043342,\n",
       "  0.0017664081975965473,\n",
       "  0.0017606183658074315,\n",
       "  0.0017547520972414077,\n",
       "  0.0017487958565709505,\n",
       "  0.001742810639285608,\n",
       "  0.0017373603716350772,\n",
       "  0.001731631371342246,\n",
       "  0.0017259345434013085,\n",
       "  0.0017199819356790158,\n",
       "  0.0017143663184242541,\n",
       "  0.0017090719281895872,\n",
       "  0.0017033968231316976,\n",
       "  0.0016978648478129799,\n",
       "  0.001692476009765187,\n",
       "  0.0016869061075226455,\n",
       "  0.0016814744218720534,\n",
       "  0.0016761830113978249,\n",
       "  0.0016708138466583478,\n",
       "  0.0016654822071018316,\n",
       "  0.001660073882476354,\n",
       "  0.0016546408779447574,\n",
       "  0.0016493998698763335,\n",
       "  0.001644350392666638,\n",
       "  0.0016391696768778274,\n",
       "  0.001634013386336619,\n",
       "  0.0016287342025975704,\n",
       "  0.0016238942435506942,\n",
       "  0.0016185844203278066,\n",
       "  0.0016138004805554829,\n",
       "  0.0016085458500754157,\n",
       "  0.0016038169543435288,\n",
       "  0.0015987700815287553,\n",
       "  0.0015938433218423143,\n",
       "  0.0015889465730651864,\n",
       "  0.0015839339317835766,\n",
       "  0.0015793303188047503,\n",
       "  0.0015742849919604396,\n",
       "  0.001569642572775365,\n",
       "  0.00156474039386012,\n",
       "  0.0015601473805020305,\n",
       "  0.001555300123982306,\n",
       "  0.0015506248587087157,\n",
       "  0.0015459728238624848,\n",
       "  0.0015415662464022262,\n",
       "  0.001536745156130337,\n",
       "  0.001532382023469836,\n",
       "  0.0015277452484099285,\n",
       "  0.0015232880238329471,\n",
       "  0.0015185713531593592,\n",
       "  0.0015140950150396293,\n",
       "  0.0015097668623127447,\n",
       "  0.0015052049138262191,\n",
       "  0.00150099951704874,\n",
       "  0.001496532407770701,\n",
       "  0.0014922438575126762,\n",
       "  0.001487823680861311,\n",
       "  0.0014835010240972224,\n",
       "  0.0014790883628279516,\n",
       "  0.00147482102278218,\n",
       "  0.0014705774115100212,\n",
       "  0.001466354557237537,\n",
       "  0.0014622687731389306,\n",
       "  0.001458154078932239,\n",
       "  0.0014539916210765114,\n",
       "  0.001449851396472468,\n",
       "  0.0014457302097241609,\n",
       "  0.0014415662512021754,\n",
       "  0.0014373753397762812,\n",
       "  0.0014334366619550565,\n",
       "  0.0014293987102291794,\n",
       "  0.0014253831643031845,\n",
       "  0.0014214589689345303,\n",
       "  0.001417486786721156,\n",
       "  0.0014135358598252878,\n",
       "  0.0014095391760520215,\n",
       "  0.0014056924328542664,\n",
       "  0.0014017352477821987,\n",
       "  0.0013977648400782527,\n",
       "  0.001394015736508436,\n",
       "  0.0013900807202276155,\n",
       "  0.0013863680638905862,\n",
       "  0.0013825764409034491,\n",
       "  0.0013788649030175293,\n",
       "  0.001375044054668706,\n",
       "  0.0013712155941902893,\n",
       "  0.001367593304763659,\n",
       "  0.0013639500181454878,\n",
       "  0.0013602100860339306,\n",
       "  0.0013566066226360363,\n",
       "  0.0013529023639762272,\n",
       "  0.0013491826320262135,\n",
       "  0.0013456646201461808,\n",
       "  0.0013419825197615864,\n",
       "  0.001338500478283322,\n",
       "  0.0013349422099793173,\n",
       "  0.001331313283401414,\n",
       "  0.0013277899410866212,\n",
       "  0.0013243745395641326,\n",
       "  0.00132079989077738,\n",
       "  0.0013174717010713266,\n",
       "  0.0013140139606093726,\n",
       "  0.0013105210603043815,\n",
       "  0.0013071522580787587,\n",
       "  0.0013037482204936168,\n",
       "  0.0013003068008856644,\n",
       "  0.0012969329098666549,\n",
       "  0.0012935751106795124,\n",
       "  0.0012902348109378181,\n",
       "  0.0012868282425131383,\n",
       "  0.0012835181530022055,\n",
       "  0.001280305836246498,\n",
       "  0.0012770771386385604,\n",
       "  0.0012738149839321977,\n",
       "  0.001270565097030385,\n",
       "  0.0012672062116223515,\n",
       "  0.0012639883599661061,\n",
       "  0.0012608638876372424,\n",
       "  0.0012576749211348215,\n",
       "  0.0012545491970854528,\n",
       "  0.0012513453277733788,\n",
       "  0.001248245216174837,\n",
       "  0.0012449962540586057,\n",
       "  0.0012418837934831117,\n",
       "  0.001238900666464815,\n",
       "  0.0012356957220447384,\n",
       "  0.0012326255726975432,\n",
       "  0.001229686663236862,\n",
       "  0.0012265271405738076,\n",
       "  0.0012235009750725112,\n",
       "  0.0012204874630867835,\n",
       "  0.0012175552756445995,\n",
       "  0.0012146068868048587,\n",
       "  0.0012115837967314746,\n",
       "  0.001208663550272604,\n",
       "  0.0012056700869322867,\n",
       "  0.0012026648408607343,\n",
       "  0.0011997460448753827,\n",
       "  0.0011969049361084672,\n",
       "  0.0011940046874485688,\n",
       "  0.0011911196723985995,\n",
       "  0.0011881805042150972,\n",
       "  0.0011853257617246982,\n",
       "  0.0011825851384230646,\n",
       "  0.0011797530867738715,\n",
       "  0.001176826975182621,\n",
       "  0.0011741257889070766,\n",
       "  0.0011712890517685537,\n",
       "  0.0011684402553839845,\n",
       "  0.0011657705982770494,\n",
       "  0.001163010934050135,\n",
       "  0.0011602242119579387,\n",
       "  0.0011575249024531331,\n",
       "  0.0011547992624514336,\n",
       "  0.0011520496467056773,\n",
       "  0.0011493857678317454,\n",
       "  0.00114660114982712,\n",
       "  0.0011439837503672834,\n",
       "  0.0011412580762595326,\n",
       "  0.0011386654296927645,\n",
       "  0.0011360583375221722,\n",
       "  0.0011334229507866667,\n",
       "  0.0011307688223780588,\n",
       "  0.0011281965371523484,\n",
       "  0.0011255050024203683,\n",
       "  0.0011230089687188638,\n",
       "  0.001120433149040423,\n",
       "  0.0011178676265615195,\n",
       "  0.00111522487542004,\n",
       "  0.0011126812016755139,\n",
       "  0.0011101497770482638,\n",
       "  0.001107716759406685,\n",
       "  0.0011051689392900244,\n",
       "  0.0011026672798521986,\n",
       "  0.0011002060183074036,\n",
       "  0.0010976413053782608,\n",
       "  0.0010952563557538042,\n",
       "  0.0010927111373888465,\n",
       "  0.0010903131542079336,\n",
       "  0.0010879054597694449,\n",
       "  0.0010854729708000832,\n",
       "  0.001083024487286366,\n",
       "  0.0010806105770342356,\n",
       "  0.0010782437056349181,\n",
       "  0.0010758508742014783,\n",
       "  0.001073471318676411,\n",
       "  0.0010710689782484906,\n",
       "  0.0010686597060782788,\n",
       "  0.0010663875140579648,\n",
       "  0.001064043505678569,\n",
       "  0.001061635016725404,\n",
       "  0.0010593593336949425,\n",
       "  0.0010570749221017195,\n",
       "  0.001054693444373811,\n",
       "  0.0010523994347551024,\n",
       "  0.0010501607192310303,\n",
       "  0.0010478392247392776,\n",
       "  0.0010456458052772177,\n",
       "  0.0010433113619967571,\n",
       "  0.0010410649348955303,\n",
       "  0.001038897610243339,\n",
       "  0.0010365945428816303,\n",
       "  0.0010344140592621898,\n",
       "  0.0010321546721598907,\n",
       "  0.001029996380736975,\n",
       "  0.0010277972145473768,\n",
       "  0.0010255631700175627,\n",
       "  0.0010233845709279876,\n",
       "  0.0010212159364613171,\n",
       "  0.0010190965364955673,\n",
       "  0.0010168984265988001,\n",
       "  0.001014755057582306,\n",
       "  0.0010126880711140984,\n",
       "  0.0010105307778189854,\n",
       "  0.001008434068452857,\n",
       "  0.0010062538631103735,\n",
       "  0.0010042183499763269,\n",
       "  0.0010021219546842985,\n",
       "  0.0010000311068125711,\n",
       "  0.0009979504668962136,\n",
       "  0.000995812968128007,\n",
       "  0.000993814367319601,\n",
       "  0.0009917587807137807,\n",
       "  0.0009897080650489443,\n",
       "  0.0009876418422578581,\n",
       "  0.0009855688717692601,\n",
       "  0.0009836085096139868,\n",
       "  0.000981564630578191,\n",
       "  0.0009795789925988174,\n",
       "  0.0009775756149537074,\n",
       "  0.0009755202939487617,\n",
       "  0.0009735340772323298,\n",
       "  0.0009716174048008952,\n",
       "  0.000969619391966603,\n",
       "  0.0009676180169719828,\n",
       "  0.0009656991103668133,\n",
       "  0.0009637452008348646,\n",
       "  0.0009618035333147645,\n",
       "  0.0009598309490712609,\n",
       "  0.0009579613403616349,\n",
       "  0.0009559837502592927,\n",
       "  0.000954104357901997,\n",
       "  0.0009521954215898631,\n",
       "  0.0009502940137338101,\n",
       "  0.0009483662185658659,\n",
       "  0.0009465393800667243,\n",
       "  0.0009446595453681539,\n",
       "  0.0009427301460078752,\n",
       "  0.0009409202035612907,\n",
       "  0.0009390614946624964,\n",
       "  0.0009371563141087336,\n",
       "  0.0009353106808731582,\n",
       "  0.0009335280271045576,\n",
       "  0.0009316435035060522,\n",
       "  0.000929818269213456,\n",
       "  0.0009280030551508389,\n",
       "  0.0009261915199942905,\n",
       "  0.0009244398798102793,\n",
       "  0.0009225884389536532,\n",
       "  0.0009208484069773704,\n",
       "  0.0009190423955374172,\n",
       "  0.0009172617276220763,\n",
       "  0.0009154539009870177,\n",
       "  0.0009137202486613205,\n",
       "  0.0009119265581460922,\n",
       "  0.0009101758970047058,\n",
       "  0.0009084773047050021,\n",
       "  0.0009067367029302886,\n",
       "  0.0009049511147338762,\n",
       "  0.0009032524727246027,\n",
       "  0.0009014984736097631,\n",
       "  0.000899828951020254,\n",
       "  0.0008981189402703649,\n",
       "  0.0008963636949204101,\n",
       "  0.000894667862866072,\n",
       "  0.0008930230848562815,\n",
       "  0.0008913152701097715,\n",
       "  0.000889604230110308,\n",
       "  0.0008879583492245047,\n",
       "  0.0008862878810305119,\n",
       "  0.0008845930889833653,\n",
       "  0.0008829664295149087,\n",
       "  0.0008813116280066424,\n",
       "  0.0008796639830848108,\n",
       "  0.000877994165648488,\n",
       "  0.0008763882201492379,\n",
       "  0.0008747585955291015,\n",
       "  0.0008731332904972078,\n",
       "  0.0008715151351736949,\n",
       "  0.0008699024230622854,\n",
       "  0.0008682670198112419,\n",
       "  0.0008667084397728786,\n",
       "  0.0008650686283289133,\n",
       "  0.0008635041731821502,\n",
       "  0.0008619199815029788,\n",
       "  0.0008603388878611895,\n",
       "  0.0008587373308017732,\n",
       "  0.0008571948675637486,\n",
       "  0.0008556307059279789,\n",
       "  0.0008540874370339679,\n",
       "  0.0008525161908892187,\n",
       "  0.0008509431199144046,\n",
       "  0.0008494269497703044,\n",
       "  0.0008479034682267184,\n",
       "  0.0008463698313221357,\n",
       "  0.0008448437132528852,\n",
       "  0.0008432795124093673,\n",
       "  0.0008418021837428569,\n",
       "  0.0008402499818913611,\n",
       "  0.0008387420316466887,\n",
       "  0.000837281120567468,\n",
       "  0.0008357671787848631,\n",
       "  0.0008342888984742558,\n",
       "  0.0008328027968786955,\n",
       "  0.0008313199447229636,\n",
       "  0.0008298036222385008,\n",
       "  0.0008283545026586125,\n",
       "  0.0008268607243451243,\n",
       "  0.0008254222727117622,\n",
       "  0.0008239408628298861,\n",
       "  0.000822511615083954,\n",
       "  0.0008210630254978923,\n",
       "  0.0008196316142488058,\n",
       "  0.0008181904419330976,\n",
       "  0.0008167179174307648,\n",
       "  0.0008153121150229481,\n",
       "  0.000813886651002349,\n",
       "  0.0008124818099588538,\n",
       "  0.0008110271781021083,\n",
       "  0.0008096515361870615,\n",
       "  0.000808245448538969,\n",
       "  0.0008068068040358867,\n",
       "  0.0008054439490905452,\n",
       "  0.0008040371029380822,\n",
       "  0.0008026268554459648,\n",
       "  0.0008012632740417303,\n",
       "  0.0007998610910675552,\n",
       "  0.00079852007419845,\n",
       "  0.0007971353033866535,\n",
       "  0.0007957683026234008,\n",
       "  0.0007943844431395064,\n",
       "  0.0007930620382836651,\n",
       "  0.0007916737165487575,\n",
       "  0.000790344893076167,\n",
       "  0.0007890120934916922,\n",
       "  0.0007876365118306694,\n",
       "  0.0007863344390841576,\n",
       "  0.0007849876026568836,\n",
       "  0.0007836708028605699,\n",
       "  0.0007823474434758282,\n",
       "  0.0007810273652353317,\n",
       "  0.0007796776128117244,\n",
       "  0.0007783672495094152,\n",
       "  0.000777091564899024,\n",
       "  0.0007757541817738911,\n",
       "  0.0007744556291697791,\n",
       "  0.0007731590216195627,\n",
       "  0.0007718882194609478,\n",
       "  0.0007706003953984464,\n",
       "  0.0007693171238969398,\n",
       "  0.0007680370203502207,\n",
       "  0.0007667601140042279,\n",
       "  0.0007654899793720915,\n",
       "  0.0007642005681418595,\n",
       "  0.0007629571818483782,\n",
       "  0.0007617076942523227,\n",
       "  0.0007604407593930773,\n",
       "  0.0007591873145571526,\n",
       "  0.0007579194545144219,\n",
       "  0.0007567046457097604,\n",
       "  0.0007554518457482118,\n",
       "  0.0007542279059435941,\n",
       "  0.0007529921500974979,\n",
       "  0.0007517514674475478,\n",
       "  0.0007505062728512257,\n",
       "  0.0007492841231027114,\n",
       "  0.0007480839091584812,\n",
       "  0.0007468799099967782,\n",
       "  0.0007456406992361053,\n",
       "  0.0007444492546113679,\n",
       "  0.0007432267274936066,\n",
       "  0.0007420563952962311,\n",
       "  0.0007408489981344246,\n",
       "  0.0007396373301986614,\n",
       "  0.000738449558866705,\n",
       "  0.0007372908823981561,\n",
       "  0.0007360798552014103,\n",
       "  0.0007349013459361231,\n",
       "  0.0007337278787815649,\n",
       "  0.0007325719906674522,\n",
       "  0.0007314037018915087,\n",
       "  0.0007302366633581414,\n",
       "  0.0007290759090068377,\n",
       "  0.0007279026070960981,\n",
       "  0.0007267461211313275,\n",
       "  0.0007256110636433277,\n",
       "  0.00072444754175649,\n",
       "  0.0007233186218513765,\n",
       "  0.0007221883776651339,\n",
       "  0.0007210499269092452,\n",
       "  0.0007199150273606847,\n",
       "  0.0007187729644897445,\n",
       "  0.0007176303032707665,\n",
       "  0.000716506436035047,\n",
       "  0.000715385106047961,\n",
       "  0.0007142669633850574,\n",
       "  0.0007131681537515533,\n",
       "  0.0007120649421921144,\n",
       "  0.0007109314890811902,\n",
       "  0.000709826646598799,\n",
       "  0.0007087485666943394,\n",
       "  0.0007076252458091403,\n",
       "  0.0007065560689056838,\n",
       "  0.0007054532603305574,\n",
       "  0.0007043642721811239,\n",
       "  0.0007032606388008844,\n",
       "  0.0007021784850564927,\n",
       "  0.0007011245009351796,\n",
       "  0.0007000481335737163,\n",
       "  0.0006989657129636839,\n",
       "  0.0006978939758113256,\n",
       "  0.0006968363970045743,\n",
       "  0.0006957604280302121,\n",
       "  0.0006946826024645247,\n",
       "  0.000693648173779593,\n",
       "  0.0006925937985329256,\n",
       "  0.0006915167957259882,\n",
       "  0.0006904825710591922,\n",
       "  0.0006894370654109139,\n",
       "  0.0006883775467017017,\n",
       "  0.0006873501504927049,\n",
       "  0.0006863220617461252,\n",
       "  0.0006852856360310966,\n",
       "  0.0006842555559956054,\n",
       "  0.0006832027459893197,\n",
       "  0.0006821984218987609,\n",
       "  0.0006811655498069058,\n",
       "  0.0006801322948675192,\n",
       "  0.0006791274857036456,\n",
       "  0.0006780997225064311,\n",
       "  0.0006771003276426768,\n",
       "  0.0006761003233322072,\n",
       "  0.0006750707204048467,\n",
       "  0.0006740823616908642,\n",
       "  0.0006730678879165453,\n",
       "  0.0006720911105659866,\n",
       "  0.0006710957949707108,\n",
       "  0.000670093813909034,\n",
       "  0.0006691047496047986,\n",
       "  0.0006681258531529011,\n",
       "  0.0006671316192594645,\n",
       "  0.000666150894815587,\n",
       "  0.0006651807221089712,\n",
       "  0.0006641815267016682,\n",
       "  0.0006632295395121446,\n",
       "  0.0006622506989373278,\n",
       "  0.0006612902064730974,\n",
       "  0.0006603239557918528,\n",
       "  0.0006593525818914259,\n",
       "  0.0006583931828174627,\n",
       "  ...],\n",
       " 'mse': [0.29656222,\n",
       "  0.2385355,\n",
       "  0.22100542,\n",
       "  0.21384442,\n",
       "  0.21007106,\n",
       "  0.20859629,\n",
       "  0.20775543,\n",
       "  0.20478377,\n",
       "  0.20751046,\n",
       "  0.20447221,\n",
       "  0.20712593,\n",
       "  0.20543507,\n",
       "  0.20548744,\n",
       "  0.20535281,\n",
       "  0.20622337,\n",
       "  0.20352253,\n",
       "  0.20606463,\n",
       "  0.20455976,\n",
       "  0.20542437,\n",
       "  0.20504965,\n",
       "  0.2046111,\n",
       "  0.20363177,\n",
       "  0.20374681,\n",
       "  0.20230019,\n",
       "  0.20399655,\n",
       "  0.2046814,\n",
       "  0.20429792,\n",
       "  0.2017123,\n",
       "  0.20431876,\n",
       "  0.20386942,\n",
       "  0.2025942,\n",
       "  0.20337328,\n",
       "  0.20095053,\n",
       "  0.2033959,\n",
       "  0.20306826,\n",
       "  0.20280093,\n",
       "  0.2004328,\n",
       "  0.20047559,\n",
       "  0.2030652,\n",
       "  0.20181973,\n",
       "  0.20237596,\n",
       "  0.20189476,\n",
       "  0.2009034,\n",
       "  0.20159993,\n",
       "  0.20062631,\n",
       "  0.20132445,\n",
       "  0.20097652,\n",
       "  0.2007332,\n",
       "  0.19838877,\n",
       "  0.19837208,\n",
       "  0.2007095,\n",
       "  0.19815041,\n",
       "  0.20044981,\n",
       "  0.2000478,\n",
       "  0.19758864,\n",
       "  0.19994682,\n",
       "  0.19854385,\n",
       "  0.19926277,\n",
       "  0.1987303,\n",
       "  0.1984523,\n",
       "  0.19821656,\n",
       "  0.19792911,\n",
       "  0.19566208,\n",
       "  0.19757706,\n",
       "  0.19513312,\n",
       "  0.19708574,\n",
       "  0.19675498,\n",
       "  0.19573087,\n",
       "  0.19556901,\n",
       "  0.19540834,\n",
       "  0.19560766,\n",
       "  0.19526075,\n",
       "  0.19278947,\n",
       "  0.19263422,\n",
       "  0.1947049,\n",
       "  0.1940335,\n",
       "  0.19331446,\n",
       "  0.19230609,\n",
       "  0.19251917,\n",
       "  0.18998843,\n",
       "  0.19151388,\n",
       "  0.1904861,\n",
       "  0.19059788,\n",
       "  0.18993033,\n",
       "  0.1893728,\n",
       "  0.18689875,\n",
       "  0.18804725,\n",
       "  0.18782951,\n",
       "  0.18704817,\n",
       "  0.18545212,\n",
       "  0.18528616,\n",
       "  0.18316093,\n",
       "  0.18412286,\n",
       "  0.18351507,\n",
       "  0.18251036,\n",
       "  0.17980866,\n",
       "  0.18068953,\n",
       "  0.1797511,\n",
       "  0.17860743,\n",
       "  0.17748876,\n",
       "  0.17698717,\n",
       "  0.17536794,\n",
       "  0.17272331,\n",
       "  0.1734849,\n",
       "  0.1723755,\n",
       "  0.17035723,\n",
       "  0.16957225,\n",
       "  0.16766493,\n",
       "  0.16646141,\n",
       "  0.16503336,\n",
       "  0.16401786,\n",
       "  0.16062213,\n",
       "  0.15994416,\n",
       "  0.15921351,\n",
       "  0.15573022,\n",
       "  0.15585403,\n",
       "  0.15308642,\n",
       "  0.15234737,\n",
       "  0.14896604,\n",
       "  0.14916788,\n",
       "  0.14718302,\n",
       "  0.14334475,\n",
       "  0.14197576,\n",
       "  0.14079669,\n",
       "  0.138473,\n",
       "  0.13742165,\n",
       "  0.13517176,\n",
       "  0.1327237,\n",
       "  0.129591,\n",
       "  0.12833764,\n",
       "  0.12548506,\n",
       "  0.12329372,\n",
       "  0.121872865,\n",
       "  0.120470494,\n",
       "  0.11892373,\n",
       "  0.11518584,\n",
       "  0.11310959,\n",
       "  0.11091549,\n",
       "  0.10887999,\n",
       "  0.10837599,\n",
       "  0.10507506,\n",
       "  0.10434003,\n",
       "  0.10201396,\n",
       "  0.09986502,\n",
       "  0.096494496,\n",
       "  0.09498253,\n",
       "  0.09332802,\n",
       "  0.09111511,\n",
       "  0.08966349,\n",
       "  0.087703206,\n",
       "  0.084399395,\n",
       "  0.08318212,\n",
       "  0.08133428,\n",
       "  0.079239964,\n",
       "  0.07765328,\n",
       "  0.07504302,\n",
       "  0.073758654,\n",
       "  0.072905414,\n",
       "  0.06984632,\n",
       "  0.06876726,\n",
       "  0.06657871,\n",
       "  0.066176295,\n",
       "  0.06381284,\n",
       "  0.062372457,\n",
       "  0.06109407,\n",
       "  0.06015088,\n",
       "  0.057854578,\n",
       "  0.055945873,\n",
       "  0.055516534,\n",
       "  0.0536181,\n",
       "  0.05281868,\n",
       "  0.05061665,\n",
       "  0.050303094,\n",
       "  0.04835263,\n",
       "  0.047503166,\n",
       "  0.045855425,\n",
       "  0.04553935,\n",
       "  0.04435978,\n",
       "  0.043251216,\n",
       "  0.042189617,\n",
       "  0.040858347,\n",
       "  0.039585296,\n",
       "  0.03896639,\n",
       "  0.037747655,\n",
       "  0.037161857,\n",
       "  0.03650847,\n",
       "  0.03542643,\n",
       "  0.034374673,\n",
       "  0.0340394,\n",
       "  0.032883603,\n",
       "  0.032370735,\n",
       "  0.03158041,\n",
       "  0.031072995,\n",
       "  0.030351115,\n",
       "  0.029530989,\n",
       "  0.029050715,\n",
       "  0.028091116,\n",
       "  0.027674362,\n",
       "  0.027194185,\n",
       "  0.026499972,\n",
       "  0.026075194,\n",
       "  0.025514953,\n",
       "  0.024881378,\n",
       "  0.024402484,\n",
       "  0.02402122,\n",
       "  0.023421269,\n",
       "  0.02305851,\n",
       "  0.022501225,\n",
       "  0.022153078,\n",
       "  0.021633048,\n",
       "  0.0212413,\n",
       "  0.020736093,\n",
       "  0.020421233,\n",
       "  0.020054419,\n",
       "  0.01975438,\n",
       "  0.019374765,\n",
       "  0.019017966,\n",
       "  0.018622532,\n",
       "  0.018349614,\n",
       "  0.018020472,\n",
       "  0.017704532,\n",
       "  0.017397484,\n",
       "  0.017099453,\n",
       "  0.016767863,\n",
       "  0.016430838,\n",
       "  0.016170025,\n",
       "  0.015924009,\n",
       "  0.015754316,\n",
       "  0.015447483,\n",
       "  0.015187711,\n",
       "  0.01497939,\n",
       "  0.014712192,\n",
       "  0.014488985,\n",
       "  0.014271863,\n",
       "  0.014058552,\n",
       "  0.013849847,\n",
       "  0.0136666475,\n",
       "  0.013426411,\n",
       "  0.013254491,\n",
       "  0.013031097,\n",
       "  0.012810146,\n",
       "  0.012623031,\n",
       "  0.012450219,\n",
       "  0.012305278,\n",
       "  0.012126144,\n",
       "  0.011930766,\n",
       "  0.011812927,\n",
       "  0.011651002,\n",
       "  0.011493124,\n",
       "  0.011300771,\n",
       "  0.011192353,\n",
       "  0.011042685,\n",
       "  0.010863393,\n",
       "  0.010728933,\n",
       "  0.010611381,\n",
       "  0.010467589,\n",
       "  0.010333719,\n",
       "  0.010217182,\n",
       "  0.010089306,\n",
       "  0.009952839,\n",
       "  0.009814783,\n",
       "  0.00971235,\n",
       "  0.009579547,\n",
       "  0.009481223,\n",
       "  0.009378704,\n",
       "  0.009244299,\n",
       "  0.009139083,\n",
       "  0.009036833,\n",
       "  0.008946944,\n",
       "  0.008849472,\n",
       "  0.008740213,\n",
       "  0.00864959,\n",
       "  0.008533718,\n",
       "  0.0084599415,\n",
       "  0.00836673,\n",
       "  0.008257773,\n",
       "  0.00818762,\n",
       "  0.008099025,\n",
       "  0.007996173,\n",
       "  0.007922228,\n",
       "  0.00782794,\n",
       "  0.0077484385,\n",
       "  0.007670494,\n",
       "  0.0076006893,\n",
       "  0.0075196656,\n",
       "  0.007434422,\n",
       "  0.007374259,\n",
       "  0.0072947727,\n",
       "  0.007227004,\n",
       "  0.0071434053,\n",
       "  0.0070816083,\n",
       "  0.0070170416,\n",
       "  0.0069492934,\n",
       "  0.006871384,\n",
       "  0.0068132617,\n",
       "  0.006741933,\n",
       "  0.006690018,\n",
       "  0.0066274544,\n",
       "  0.0065560364,\n",
       "  0.0065063476,\n",
       "  0.0064424914,\n",
       "  0.006387017,\n",
       "  0.0063293297,\n",
       "  0.0062636947,\n",
       "  0.006208843,\n",
       "  0.0061594257,\n",
       "  0.006104238,\n",
       "  0.0060539376,\n",
       "  0.0059981574,\n",
       "  0.005941565,\n",
       "  0.005898622,\n",
       "  0.005848349,\n",
       "  0.0057987534,\n",
       "  0.005747039,\n",
       "  0.0056988765,\n",
       "  0.0056472775,\n",
       "  0.005604822,\n",
       "  0.005554759,\n",
       "  0.005516149,\n",
       "  0.0054713506,\n",
       "  0.005427178,\n",
       "  0.005377573,\n",
       "  0.005338617,\n",
       "  0.005298315,\n",
       "  0.0052508777,\n",
       "  0.005215704,\n",
       "  0.0051696054,\n",
       "  0.0051301075,\n",
       "  0.005091163,\n",
       "  0.0050576297,\n",
       "  0.0050138948,\n",
       "  0.004979062,\n",
       "  0.0049407077,\n",
       "  0.004903266,\n",
       "  0.0048666215,\n",
       "  0.004830609,\n",
       "  0.0047923336,\n",
       "  0.004761755,\n",
       "  0.004722702,\n",
       "  0.0046929694,\n",
       "  0.0046550063,\n",
       "  0.0046260273,\n",
       "  0.004589039,\n",
       "  0.004560698,\n",
       "  0.0045246533,\n",
       "  0.0044932887,\n",
       "  0.0044643213,\n",
       "  0.004430717,\n",
       "  0.004403966,\n",
       "  0.0043701367,\n",
       "  0.0043424913,\n",
       "  0.0043105977,\n",
       "  0.004285024,\n",
       "  0.0042560794,\n",
       "  0.0042274897,\n",
       "  0.0041979738,\n",
       "  0.0041681076,\n",
       "  0.004140867,\n",
       "  0.004116913,\n",
       "  0.0040870477,\n",
       "  0.0040624,\n",
       "  0.004037007,\n",
       "  0.0040110005,\n",
       "  0.003984173,\n",
       "  0.003958668,\n",
       "  0.0039319084,\n",
       "  0.003908822,\n",
       "  0.003884241,\n",
       "  0.0038584233,\n",
       "  0.0038360152,\n",
       "  0.0038123028,\n",
       "  0.0037888563,\n",
       "  0.0037666373,\n",
       "  0.0037427042,\n",
       "  0.0037200698,\n",
       "  0.0036985462,\n",
       "  0.003676232,\n",
       "  0.0036519011,\n",
       "  0.0036323352,\n",
       "  0.0036107858,\n",
       "  0.003587324,\n",
       "  0.0035663287,\n",
       "  0.0035467912,\n",
       "  0.0035260562,\n",
       "  0.0035055995,\n",
       "  0.003486155,\n",
       "  0.003466109,\n",
       "  0.0034462754,\n",
       "  0.0034247509,\n",
       "  0.0034072327,\n",
       "  0.0033879937,\n",
       "  0.003368949,\n",
       "  0.0033500998,\n",
       "  0.003330736,\n",
       "  0.0033129135,\n",
       "  0.0032929124,\n",
       "  0.0032748813,\n",
       "  0.003257036,\n",
       "  0.0032409693,\n",
       "  0.0032233424,\n",
       "  0.003205259,\n",
       "  0.0031885356,\n",
       "  0.0031708286,\n",
       "  0.0031538587,\n",
       "  0.003137094,\n",
       "  0.0031204992,\n",
       "  0.0031046239,\n",
       "  0.0030877618,\n",
       "  0.0030707568,\n",
       "  0.003055605,\n",
       "  0.0030397857,\n",
       "  0.0030241145,\n",
       "  0.0030077398,\n",
       "  0.002993114,\n",
       "  0.0029770504,\n",
       "  0.0029626992,\n",
       "  0.002946938,\n",
       "  0.0029328465,\n",
       "  0.0029181256,\n",
       "  0.0029027935,\n",
       "  0.0028895235,\n",
       "  0.0028747164,\n",
       "  0.0028609682,\n",
       "  0.0028468694,\n",
       "  0.0028317627,\n",
       "  0.0028186291,\n",
       "  0.0028053357,\n",
       "  0.0027917249,\n",
       "  0.002778242,\n",
       "  0.0027644506,\n",
       "  0.0027516095,\n",
       "  0.0027374302,\n",
       "  0.0027254506,\n",
       "  0.002712528,\n",
       "  0.0026997218,\n",
       "  0.002687017,\n",
       "  0.0026734604,\n",
       "  0.0026615853,\n",
       "  0.002649553,\n",
       "  0.0026372736,\n",
       "  0.0026247264,\n",
       "  0.0026130066,\n",
       "  0.0026001232,\n",
       "  0.0025888134,\n",
       "  0.0025773696,\n",
       "  0.0025656803,\n",
       "  0.0025532313,\n",
       "  0.00254177,\n",
       "  0.0025309024,\n",
       "  0.0025198916,\n",
       "  0.0025086647,\n",
       "  0.002497215,\n",
       "  0.002485677,\n",
       "  0.0024747502,\n",
       "  0.0024639189,\n",
       "  0.002453934,\n",
       "  0.0024424763,\n",
       "  0.0024319065,\n",
       "  0.0024214191,\n",
       "  0.0024114323,\n",
       "  0.002400582,\n",
       "  0.0023907446,\n",
       "  0.002380491,\n",
       "  0.0023703207,\n",
       "  0.002360247,\n",
       "  0.0023502638,\n",
       "  0.0023403633,\n",
       "  0.0023305414,\n",
       "  0.0023207972,\n",
       "  0.002311129,\n",
       "  0.0023017852,\n",
       "  0.002291999,\n",
       "  0.0022825473,\n",
       "  0.0022731721,\n",
       "  0.002263865,\n",
       "  0.0022542297,\n",
       "  0.0022454034,\n",
       "  0.0022359157,\n",
       "  0.0022268463,\n",
       "  0.0022184472,\n",
       "  0.0022089398,\n",
       "  0.0022006722,\n",
       "  0.002191307,\n",
       "  0.002182598,\n",
       "  0.0021745048,\n",
       "  0.0021656838,\n",
       "  0.0021571233,\n",
       "  0.002148639,\n",
       "  0.0021404265,\n",
       "  0.0021320619,\n",
       "  0.0021237568,\n",
       "  0.0021155118,\n",
       "  0.0021068123,\n",
       "  0.0020991995,\n",
       "  0.0020911251,\n",
       "  0.0020831074,\n",
       "  0.0020751425,\n",
       "  0.002066752,\n",
       "  0.0020593985,\n",
       "  0.0020514084,\n",
       "  0.0020438358,\n",
       "  0.002035963,\n",
       "  0.0020285018,\n",
       "  0.0020207285,\n",
       "  0.0020131953,\n",
       "  0.002005884,\n",
       "  0.0019984501,\n",
       "  0.0019908953,\n",
       "  0.0019835583,\n",
       "  0.001976443,\n",
       "  0.0019687752,\n",
       "  0.001961847,\n",
       "  0.0019548642,\n",
       "  0.0019476055,\n",
       "  0.0019405639,\n",
       "  0.0019335635,\n",
       "  0.0019263626,\n",
       "  0.0019198484,\n",
       "  0.0019128257,\n",
       "  0.0019057656,\n",
       "  0.0018989849,\n",
       "  0.0018922545,\n",
       "  0.0018858035,\n",
       "  0.0018789257,\n",
       "  0.0018723272,\n",
       "  0.0018657716,\n",
       "  0.0018594807,\n",
       "  0.0018531331,\n",
       "  0.001846689,\n",
       "  0.001839942,\n",
       "  0.0018335918,\n",
       "  0.0018272871,\n",
       "  0.0018213606,\n",
       "  0.0018149776,\n",
       "  0.0018088978,\n",
       "  0.0018025995,\n",
       "  0.0017965962,\n",
       "  0.0017903779,\n",
       "  0.0017843217,\n",
       "  0.0017781131,\n",
       "  0.0017721442,\n",
       "  0.0017664082,\n",
       "  0.0017606184,\n",
       "  0.001754752,\n",
       "  0.0017487959,\n",
       "  0.0017428106,\n",
       "  0.0017373604,\n",
       "  0.0017316315,\n",
       "  0.0017259346,\n",
       "  0.001719982,\n",
       "  0.0017143664,\n",
       "  0.0017090719,\n",
       "  0.0017033968,\n",
       "  0.0016978648,\n",
       "  0.001692476,\n",
       "  0.0016869062,\n",
       "  0.0016814745,\n",
       "  0.001676183,\n",
       "  0.0016708139,\n",
       "  0.0016654822,\n",
       "  0.0016600739,\n",
       "  0.0016546408,\n",
       "  0.0016493999,\n",
       "  0.0016443505,\n",
       "  0.0016391696,\n",
       "  0.0016340134,\n",
       "  0.0016287342,\n",
       "  0.0016238942,\n",
       "  0.0016185844,\n",
       "  0.0016138004,\n",
       "  0.0016085459,\n",
       "  0.001603817,\n",
       "  0.00159877,\n",
       "  0.0015938433,\n",
       "  0.0015889467,\n",
       "  0.0015839338,\n",
       "  0.0015793303,\n",
       "  0.0015742849,\n",
       "  0.0015696426,\n",
       "  0.0015647403,\n",
       "  0.0015601474,\n",
       "  0.0015553001,\n",
       "  0.0015506248,\n",
       "  0.0015459729,\n",
       "  0.0015415661,\n",
       "  0.0015367452,\n",
       "  0.001532382,\n",
       "  0.0015277453,\n",
       "  0.001523288,\n",
       "  0.0015185714,\n",
       "  0.0015140951,\n",
       "  0.0015097669,\n",
       "  0.001505205,\n",
       "  0.0015009996,\n",
       "  0.0014965325,\n",
       "  0.0014922438,\n",
       "  0.0014878237,\n",
       "  0.0014835009,\n",
       "  0.0014790883,\n",
       "  0.0014748211,\n",
       "  0.0014705774,\n",
       "  0.0014663546,\n",
       "  0.0014622688,\n",
       "  0.0014581541,\n",
       "  0.0014539916,\n",
       "  0.0014498514,\n",
       "  0.0014457302,\n",
       "  0.0014415663,\n",
       "  0.0014373753,\n",
       "  0.0014334368,\n",
       "  0.0014293988,\n",
       "  0.0014253831,\n",
       "  0.001421459,\n",
       "  0.0014174867,\n",
       "  0.0014135359,\n",
       "  0.0014095391,\n",
       "  0.0014056924,\n",
       "  0.0014017352,\n",
       "  0.0013977649,\n",
       "  0.0013940157,\n",
       "  0.0013900807,\n",
       "  0.0013863681,\n",
       "  0.0013825764,\n",
       "  0.001378865,\n",
       "  0.001375044,\n",
       "  0.0013712156,\n",
       "  0.0013675933,\n",
       "  0.00136395,\n",
       "  0.00136021,\n",
       "  0.0013566066,\n",
       "  0.0013529024,\n",
       "  0.0013491827,\n",
       "  0.0013456646,\n",
       "  0.0013419825,\n",
       "  0.0013385005,\n",
       "  0.0013349422,\n",
       "  0.0013313133,\n",
       "  0.00132779,\n",
       "  0.0013243746,\n",
       "  0.0013207998,\n",
       "  0.0013174717,\n",
       "  0.001314014,\n",
       "  0.001310521,\n",
       "  0.0013071523,\n",
       "  0.0013037482,\n",
       "  0.0013003068,\n",
       "  0.0012969329,\n",
       "  0.0012935752,\n",
       "  0.0012902347,\n",
       "  0.0012868282,\n",
       "  0.0012835182,\n",
       "  0.0012803059,\n",
       "  0.0012770771,\n",
       "  0.001273815,\n",
       "  0.0012705651,\n",
       "  0.0012672062,\n",
       "  0.0012639883,\n",
       "  0.0012608638,\n",
       "  0.0012576749,\n",
       "  0.0012545491,\n",
       "  0.0012513453,\n",
       "  0.0012482452,\n",
       "  0.0012449962,\n",
       "  0.0012418837,\n",
       "  0.0012389006,\n",
       "  0.0012356958,\n",
       "  0.0012326256,\n",
       "  0.0012296867,\n",
       "  0.0012265272,\n",
       "  0.0012235009,\n",
       "  0.0012204875,\n",
       "  0.0012175553,\n",
       "  0.0012146069,\n",
       "  0.0012115838,\n",
       "  0.0012086636,\n",
       "  0.0012056701,\n",
       "  0.0012026649,\n",
       "  0.001199746,\n",
       "  0.0011969049,\n",
       "  0.0011940046,\n",
       "  0.0011911197,\n",
       "  0.0011881805,\n",
       "  0.0011853258,\n",
       "  0.0011825851,\n",
       "  0.0011797531,\n",
       "  0.0011768269,\n",
       "  0.0011741258,\n",
       "  0.001171289,\n",
       "  0.0011684403,\n",
       "  0.0011657706,\n",
       "  0.001163011,\n",
       "  0.0011602242,\n",
       "  0.0011575249,\n",
       "  0.0011547992,\n",
       "  0.0011520498,\n",
       "  0.0011493857,\n",
       "  0.0011466012,\n",
       "  0.0011439838,\n",
       "  0.001141258,\n",
       "  0.0011386655,\n",
       "  0.0011360584,\n",
       "  0.001133423,\n",
       "  0.0011307688,\n",
       "  0.0011281965,\n",
       "  0.001125505,\n",
       "  0.001123009,\n",
       "  0.0011204332,\n",
       "  0.0011178677,\n",
       "  0.0011152248,\n",
       "  0.0011126811,\n",
       "  0.0011101498,\n",
       "  0.0011077167,\n",
       "  0.001105169,\n",
       "  0.0011026673,\n",
       "  0.001100206,\n",
       "  0.0010976413,\n",
       "  0.0010952564,\n",
       "  0.0010927111,\n",
       "  0.0010903131,\n",
       "  0.0010879054,\n",
       "  0.001085473,\n",
       "  0.0010830245,\n",
       "  0.0010806106,\n",
       "  0.0010782437,\n",
       "  0.0010758509,\n",
       "  0.0010734713,\n",
       "  0.0010710689,\n",
       "  0.0010686597,\n",
       "  0.0010663874,\n",
       "  0.0010640435,\n",
       "  0.001061635,\n",
       "  0.0010593593,\n",
       "  0.0010570749,\n",
       "  0.0010546934,\n",
       "  0.0010523994,\n",
       "  0.0010501607,\n",
       "  0.0010478392,\n",
       "  0.0010456458,\n",
       "  0.0010433113,\n",
       "  0.0010410649,\n",
       "  0.0010388975,\n",
       "  0.0010365946,\n",
       "  0.001034414,\n",
       "  0.0010321547,\n",
       "  0.0010299964,\n",
       "  0.0010277972,\n",
       "  0.0010255631,\n",
       "  0.0010233845,\n",
       "  0.0010212159,\n",
       "  0.0010190965,\n",
       "  0.0010168984,\n",
       "  0.0010147551,\n",
       "  0.001012688,\n",
       "  0.0010105307,\n",
       "  0.001008434,\n",
       "  0.0010062539,\n",
       "  0.0010042184,\n",
       "  0.001002122,\n",
       "  0.0010000311,\n",
       "  0.0009979504,\n",
       "  0.0009958129,\n",
       "  0.0009938143,\n",
       "  0.0009917588,\n",
       "  0.000989708,\n",
       "  0.0009876419,\n",
       "  0.000985569,\n",
       "  0.0009836085,\n",
       "  0.0009815646,\n",
       "  0.000979579,\n",
       "  0.0009775757,\n",
       "  0.00097552035,\n",
       "  0.0009735341,\n",
       "  0.0009716174,\n",
       "  0.0009696194,\n",
       "  0.000967618,\n",
       "  0.0009656991,\n",
       "  0.0009637452,\n",
       "  0.00096180354,\n",
       "  0.00095983094,\n",
       "  0.0009579613,\n",
       "  0.00095598376,\n",
       "  0.00095410435,\n",
       "  0.0009521954,\n",
       "  0.000950294,\n",
       "  0.0009483662,\n",
       "  0.0009465394,\n",
       "  0.00094465946,\n",
       "  0.00094273017,\n",
       "  0.00094092014,\n",
       "  0.0009390615,\n",
       "  0.0009371563,\n",
       "  0.0009353107,\n",
       "  0.00093352806,\n",
       "  0.00093164353,\n",
       "  0.00092981826,\n",
       "  0.00092800305,\n",
       "  0.00092619157,\n",
       "  0.0009244399,\n",
       "  0.0009225884,\n",
       "  0.0009208484,\n",
       "  0.0009190424,\n",
       "  0.0009172617,\n",
       "  0.00091545394,\n",
       "  0.0009137202,\n",
       "  0.0009119265,\n",
       "  0.00091017596,\n",
       "  0.00090847735,\n",
       "  0.00090673665,\n",
       "  0.0009049511,\n",
       "  0.0009032525,\n",
       "  0.0009014985,\n",
       "  0.00089982897,\n",
       "  0.00089811895,\n",
       "  0.0008963637,\n",
       "  0.0008946679,\n",
       "  0.00089302304,\n",
       "  0.0008913153,\n",
       "  0.0008896043,\n",
       "  0.00088795833,\n",
       "  0.0008862879,\n",
       "  0.0008845931,\n",
       "  0.00088296644,\n",
       "  0.0008813116,\n",
       "  0.00087966403,\n",
       "  0.00087799417,\n",
       "  0.0008763882,\n",
       "  0.0008747586,\n",
       "  0.0008731333,\n",
       "  0.00087151513,\n",
       "  0.00086990243,\n",
       "  0.0008682671,\n",
       "  0.0008667084,\n",
       "  0.00086506864,\n",
       "  0.00086350413,\n",
       "  0.00086191995,\n",
       "  0.0008603389,\n",
       "  0.00085873733,\n",
       "  0.0008571948,\n",
       "  0.00085563073,\n",
       "  0.0008540874,\n",
       "  0.0008525162,\n",
       "  0.0008509431,\n",
       "  0.00084942696,\n",
       "  0.00084790343,\n",
       "  0.00084636983,\n",
       "  0.0008448437,\n",
       "  0.0008432795,\n",
       "  0.00084180216,\n",
       "  0.00084024994,\n",
       "  0.000838742,\n",
       "  0.0008372811,\n",
       "  0.00083576713,\n",
       "  0.0008342889,\n",
       "  0.0008328028,\n",
       "  0.00083131995,\n",
       "  0.0008298036,\n",
       "  0.0008283545,\n",
       "  0.0008268608,\n",
       "  0.0008254223,\n",
       "  0.0008239409,\n",
       "  0.0008225116,\n",
       "  0.00082106306,\n",
       "  0.0008196316,\n",
       "  0.00081819046,\n",
       "  0.0008167179,\n",
       "  0.0008153121,\n",
       "  0.00081388664,\n",
       "  0.0008124818,\n",
       "  0.0008110272,\n",
       "  0.00080965157,\n",
       "  0.00080824544,\n",
       "  0.0008068068,\n",
       "  0.00080544397,\n",
       "  0.0008040371,\n",
       "  0.00080262683,\n",
       "  0.00080126326,\n",
       "  0.0007998611,\n",
       "  0.00079852005,\n",
       "  0.0007971353,\n",
       "  0.00079576834,\n",
       "  0.00079438445,\n",
       "  0.00079306203,\n",
       "  0.0007916737,\n",
       "  0.0007903449,\n",
       "  0.00078901206,\n",
       "  0.0007876365,\n",
       "  0.00078633445,\n",
       "  0.0007849876,\n",
       "  0.0007836708,\n",
       "  0.0007823474,\n",
       "  0.00078102737,\n",
       "  0.0007796776,\n",
       "  0.0007783672,\n",
       "  0.00077709154,\n",
       "  0.00077575416,\n",
       "  0.0007744556,\n",
       "  0.00077315903,\n",
       "  0.00077188824,\n",
       "  0.00077060034,\n",
       "  0.00076931715,\n",
       "  0.00076803705,\n",
       "  0.0007667601,\n",
       "  0.00076549,\n",
       "  0.0007642006,\n",
       "  0.00076295715,\n",
       "  0.0007617077,\n",
       "  0.0007604408,\n",
       "  0.0007591873,\n",
       "  0.00075791945,\n",
       "  0.00075670466,\n",
       "  0.00075545185,\n",
       "  0.00075422786,\n",
       "  0.0007529921,\n",
       "  0.0007517515,\n",
       "  0.00075050624,\n",
       "  0.00074928405,\n",
       "  0.0007480839,\n",
       "  0.0007468799,\n",
       "  0.00074564066,\n",
       "  0.00074444927,\n",
       "  0.00074322673,\n",
       "  0.00074205635,\n",
       "  0.000740849,\n",
       "  0.0007396373,\n",
       "  0.00073844957,\n",
       "  0.0007372909,\n",
       "  0.0007360799,\n",
       "  0.0007349013,\n",
       "  0.0007337279,\n",
       "  0.000732572,\n",
       "  0.0007314037,\n",
       "  0.00073023664,\n",
       "  0.00072907587,\n",
       "  0.00072790263,\n",
       "  0.0007267461,\n",
       "  0.00072561105,\n",
       "  0.00072444754,\n",
       "  0.00072331866,\n",
       "  0.0007221884,\n",
       "  0.0007210499,\n",
       "  0.000719915,\n",
       "  0.000718773,\n",
       "  0.00071763026,\n",
       "  0.0007165064,\n",
       "  0.00071538513,\n",
       "  0.0007142669,\n",
       "  0.0007131682,\n",
       "  0.00071206497,\n",
       "  0.0007109315,\n",
       "  0.00070982665,\n",
       "  0.0007087485,\n",
       "  0.00070762524,\n",
       "  0.0007065561,\n",
       "  0.0007054532,\n",
       "  0.00070436427,\n",
       "  0.00070326065,\n",
       "  0.0007021785,\n",
       "  0.00070112455,\n",
       "  0.0007000481,\n",
       "  0.00069896574,\n",
       "  0.00069789396,\n",
       "  0.0006968364,\n",
       "  0.0006957605,\n",
       "  0.00069468265,\n",
       "  0.0006936482,\n",
       "  0.0006925938,\n",
       "  0.00069151685,\n",
       "  0.00069048256,\n",
       "  0.00068943703,\n",
       "  0.00068837753,\n",
       "  0.00068735017,\n",
       "  0.00068632205,\n",
       "  0.00068528566,\n",
       "  0.0006842556,\n",
       "  0.00068320276,\n",
       "  0.00068219844,\n",
       "  0.00068116555,\n",
       "  0.0006801323,\n",
       "  0.0006791275,\n",
       "  0.00067809975,\n",
       "  0.0006771003,\n",
       "  0.00067610026,\n",
       "  0.00067507074,\n",
       "  0.0006740823,\n",
       "  0.0006730679,\n",
       "  0.0006720911,\n",
       "  0.0006710958,\n",
       "  0.0006700938,\n",
       "  0.00066910475,\n",
       "  0.00066812587,\n",
       "  0.0006671316,\n",
       "  0.0006661509,\n",
       "  0.00066518073,\n",
       "  0.0006641815,\n",
       "  0.0006632295,\n",
       "  0.0006622507,\n",
       "  0.0006612902,\n",
       "  0.00066032395,\n",
       "  0.0006593526,\n",
       "  0.0006583932,\n",
       "  ...]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ddd759",
   "metadata": {},
   "source": [
    "## Well, now let us check the output as well as the new weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43d589e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/1 [========================================================================================================================] - 0s 799us/sample - loss: 3.8288e-05 - mse: 3.8288e-05\n",
      "[3.8288468203973025e-05, 3.828847e-05]\n"
     ]
    }
   ],
   "source": [
    "out = model.evaluate(inputs, outputAND, batch_size=4, verbose=1)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a02b2f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Weights for layer  1\n",
      "[[-0.06779319  3.6265917  -1.9713066 ]\n",
      " [ 0.05656172  3.6560497  -1.9379442 ]]\n",
      "\n",
      " Biases for layer  1\n",
      "[ 0.7420185 -5.2531877  2.5809631]\n",
      "\n",
      " Weights for layer  2\n",
      "[[-0.3004699]\n",
      " [ 8.487757 ]\n",
      " [-4.293654 ]]\n",
      "\n",
      " Biases for layer  2\n",
      "[-2.5681367]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(nLayers):\n",
    "    print('\\n Weights for layer ',i+1)\n",
    "    print(model.layers[i].get_weights()[0])\n",
    "    print('\\n Biases for layer ',i+1)\n",
    "    print(model.layers[i].get_weights()[1])\n",
    "# model.layers[0].get_biases()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1cf702",
   "metadata": {},
   "source": [
    "## Now also let us have a look at the predictions for the sake of the tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3e0f2c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00120628]\n",
      " [0.01536182]\n",
      " [0.01545319]\n",
      " [0.97878754]]\n"
     ]
    }
   ],
   "source": [
    "# make probability predictions with the model\n",
    "predictions = model.predict(inputs)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bbf9d7",
   "metadata": {},
   "source": [
    "## We have seen how to initialize custom weights/biases, perform forward feed, train/optimize the model, and finally how to check the updated parameters.\n",
    "\n",
    "\n",
    "## Let us now see how we can see the input/output at each layer for debugging purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "683e9b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Inputs for layer  1\n",
      "Tensor(\"dense_4_input:0\", shape=(None, 2), dtype=float32)\n",
      "\n",
      " Outputs for layer  1\n",
      "Tensor(\"dense_4/Identity:0\", shape=(None, 3), dtype=float32)\n",
      "\n",
      " Inputs for layer  2\n",
      "Tensor(\"dense_4/Identity:0\", shape=(None, 3), dtype=float32)\n",
      "\n",
      " Outputs for layer  2\n",
      "Tensor(\"dense_5/Identity:0\", shape=(None, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(nLayers):\n",
    "    print('\\n Inputs for layer ',i+1)\n",
    "    print(model.layers[i].input)\n",
    "    print('\\n Outputs for layer ',i+1)\n",
    "    print(model.layers[i].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1ddcb332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Inputs for layer  1\n",
      "\n",
      " Outputs for layer  1\n",
      "Tensor(\"dense_4/Identity:0\", shape=(None, 3), dtype=float32)\n",
      "\n",
      " Inputs for layer  2\n",
      "\n",
      " Outputs for layer  2\n",
      "Tensor(\"dense_5/Identity:0\", shape=(None, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(nLayers):\n",
    "    print('\\n Inputs for layer ',i+1)\n",
    "    print(model.layers[i].input)\n",
    "    print('\\n Outputs for layer ',i+1)\n",
    "    print(model.layers[i].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03f3fe5",
   "metadata": {},
   "source": [
    "## We can't really see any numbers in the above output\n",
    "## So we need to do something more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d6733853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Inputs for layer  1\n",
      "[[0. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 1.]]\n",
      "\n",
      " Outputs for layer  1\n",
      "[[0.67743707 0.0052036  0.9296263 ]\n",
      " [0.6896707  0.16838202 0.65543556]\n",
      " [0.66244864 0.1642972  0.64786243]\n",
      " [0.67497796 0.88385504 0.20944276]]\n",
      "\n",
      " Inputs for layer  2\n",
      "[[0.67743707 0.0052036  0.9296263 ]\n",
      " [0.6896707  0.16838202 0.65543556]\n",
      " [0.66244864 0.1642972  0.64786243]\n",
      " [0.67497796 0.88385504 0.20944276]]\n",
      "\n",
      " Outputs for layer  2\n",
      "[[0.00120628]\n",
      " [0.01536182]\n",
      " [0.01545319]\n",
      " [0.97878754]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(nLayers):\n",
    "    print('\\n Inputs for layer ',i+1)\n",
    "    func = K.function([model.get_layer(index=0).input], model.get_layer(index=i).input)\n",
    "    layerInput = func([inputs])  # input_data is a numpy array\n",
    "    print(layerInput)\n",
    "    print('\\n Outputs for layer ',i+1)\n",
    "    func = K.function([model.get_layer(index=0).input], model.get_layer(index=i).output)\n",
    "    layerOutput = func([inputs])  # input_data is a numpy array\n",
    "    print(layerOutput)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cefb9c",
   "metadata": {},
   "source": [
    "## While the outputs match my own results, the inputs don't seem to be making much sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fd8708",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
